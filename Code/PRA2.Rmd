---
title: 'Práctica 2: ¿Cómo realizar la limpieza y análisis de datos?'
subtitle: 'Tipología y ciclo de vida de los datos'
author: "Autores: Ouassim Aouattah Akandouch, Juan Andrés Dávila"
date: "Enero 2023"
output:
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
  html_document: 
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    includes:
      in_header: "75.584-PEC-header.html"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r load_libraries, include=FALSE}
library(stringr)
library(ggplot2)
library(dplyr)
library(grid)
library(gridExtra)
library(ggpubr)
library(nortest)
library(car)
```

******


# Descripción del dataset ¿Por qué es importante y qué pregunta/problema pretende responder?
******

Este dataset, llamado *Heart Attack Analysis & Prediction Dataset*, contiene información sobre pacientes que han sufrido o podrían sufrir un ataque al corazón. Incluye atributos como la edad y el sexo del paciente, si ha experimentado angina inducida por ejercicio, el número de vasos principales, el tipo de dolor en el pecho, la presión arterial en reposo, el colesterol, los resultados electrocardiográficos en reposo, la frecuencia cardíaca máxima lograda, y finalmente si tiene más o menos probabilidades de sufrir un ataque al corazón.

Este dataset es importante ya que el ataque al corazón es una de las principales causas de mortalidad a nivel mundial. El análisis de estos datos puede ayudar a identificar patrones y factores de riesgo relacionados con los ataques cardíacos, lo que permitiría a los médicos y científicos de la salud tomar medidas preventivas y mejorar los tratamientos.Se puede responder preguntas como ¿Qué factor o variables son los de mayor incidencia cuando una persona sufre de un ataque al corazón?o ¿Las mujeres son mas propensas a tener este tipo de enfermedades? ¿es el colesterol un factor más determinante para un ataque cardiaco que la edad de la persona?

Revisamos la descripción de las variables contenidas en el fichero y los tipos de cada una y a continuación Construimos un pequeño diccionario de datos utilizando la documentación auxiliar.

- `target`: Probabilidades de sufrir un ataque al corazón (0 = menor probabilidad, 1 = mayor probabilidad).
- `Age`: Edad del paciente (en años).
- `trtbps`: Presión arterial en reposo (en mm Hg).
- `cp`: Tipo de dolor en el pecho.
  - Valor 1: Angina típica.
  - Valor 2: Angina atípica.
  - Valor 3: Dolor no anginal.
  - Valor 4: Asintomático.
- `thalach`: Frecuencia cardíaca máxima alcanzada (en latidos por minuto).
- `Sex`: Sexo del paciente (1 = hombre, 0 = mujer).
- `exang`: Angina inducida por ejercicio (1 = sí, 0 = no).
- `caa`: Número de vasos principales (valores posibles: 0-3)
- `chol`: Colesterol (en mg/dl) obtenido a través del sensor de índice de masa corporal (IMC).
- `fbs`: Azúcar en sangre en ayuno > 120 mg/dl (1 = verdadero, 0 = falso).
- `rest_ecg`: Resultados electrocardiográficos en reposo.
  - Valor 0: Normal.
  - Valor 1: Anormalidades en las ondas ST-T (inversiones de onda T y/o elevación o depresión ST > 0.05 mV).
  - Valor 2: Hipertrofia ventricular izquierda probable o definitiva según los criterios de Estes.

## Objetivos

Al analizar el dataset "Heart Attack Analysis & Prediction Dataset", se pueden establecer varios objetivos metodológicos:

1. Análisis exploratorio de los datos: estudiar las variables y los patrones de los datos para obtener una comprensión general de los datos y detectar posibles problemas o outliers.

2. Identificar factores de riesgo: utilizar técnicas estadísticas para identificar variables que estén relacionadas con un mayor riesgo de sufrir un ataque al corazón.

3. Desarrollar modelos de predicción: utilizar algoritmos de aprendizaje automático para desarrollar modelos que puedan predecir el riesgo de sufrir un ataque al corazón en base a los datos de los pacientes.

4. Evaluar el rendimiento de los modelos: medir la precisión y el rendimiento de los modelos desarrollados para evaluar su capacidad para predecir el riesgo de sufrir un ataque al corazón.

5. Identificar las variables más importantes: Utilizar técnicas de selección de características para identificar las variables más importantes en la predicción de riesgo de un ataque cardíaco.

6. Utilizar el modelo para la toma de decisiones: Utilizar el modelo generado para la toma de decisiones clínicas y asesoramiento a pacientes.

******
# Integración y selección
******

Como se indicó en el apartado anterior existen muchas variables con las cuales se pueden trabajar para poder realizar un análisis por lo que empezaremos cargando el fichero de datos.

```{r}
path = 'heart.csv'
df <- read.csv(path, row.names=NULL)
```
  
Verificamos la estructura del juego de datos principal. Mostramos el número de columnas que tenemos y ejemplos de los contenidos de las filas.

```{r}
structure = str(df)
summary(df)
```

Vemos que tenemos **14** variables y **303** registros.

******
# Limpieza de los datos
******

## Valores nulos o vacíos

Un paso esencial será la limpieza de datos, mirando si hay valores vacíos o nulos.

```{r}
colSums(is.na(df))
colSums(df == "")
```

Vemos que no hay valores nulos en los datos ni existen campos llenos de espacios en blanco. Procedemos a convertir a factores las variables correspondientes a este tipo.

```{r}
df$sex <- factor(df$sex, levels = c(0, 1), labels = c("Female", "Male"))
df$exng <- factor(df$exng, levels = c(0, 1), labels = c("No", "Yes"))
df$cp <- factor(df$cp, levels = c(0, 1, 2, 3), labels = c("Typical Angina", "Atypical Angina", "Non-anginal pain", "Asymptomatic"))
df$fbs <- factor(df$fbs, levels = c(0, 1), labels = c("False", "True"))
df$restecg <- factor(df$restecg, levels = c(0, 1, 2), labels = c("Normal", "ST-T wave abnormality", "Left ventricular hypertrophy"))
df$thall <- factor(df$thall)
```

## Valores extremos

A continuación, analizaremos los posibles valores extremos de nuestro juego de datos.

```{r}
par(mfrow = c(2,2))

boxplot(df$trtbps)
plot(df$trtbps, )

boxplot(df$chol)
plot(df$chol, )

boxplot(df$oldpeak)
plot(df$oldpeak, )

boxplot(df$caa)
plot(df$caa, )
```

```{r}
# Cálculo del rango intercuartil (iqr) para cada variable
iqr_trtbps <- quantile(df$trtbps, 0.75) - quantile(df$trtbps, 0.25)
iqr_chol <- quantile(df$chol, 0.75) - quantile(df$chol, 0.25)
iqr_oldpeak <- quantile(df$oldpeak, 0.75) - quantile(df$oldpeak, 0.25)
iqr_caa <- quantile(df$caa, 0.75) - quantile(df$caa, 0.25)

# Cálculo de los límites superior e inferior para cada variable
upper_trtbps <- quantile(df$trtbps, 0.75) + 1.5*iqr_trtbps
lower_trtbps <- quantile(df$trtbps, 0.25) - 1.5*iqr_trtbps
upper_chol <- quantile(df$chol, 0.75) + 1.5*iqr_chol
lower_chol <- quantile(df$chol, 0.25) - 1.5*iqr_chol
upper_oldpeak <- quantile(df$oldpeak, 0.75) + 1.5*iqr_oldpeak
lower_oldpeak <- quantile(df$oldpeak, 0.25) - 1.5*iqr_oldpeak
upper_caa <- quantile(df$caa, 0.75) + 1.5*iqr_caa
lower_caa <- quantile(df$caa, 0.25) - 1.5*iqr_caa

# Eliminar los valores atípicos
df <- df[df$trtbps <= upper_trtbps & df$trtbps >= lower_trtbps,]
df <- df[df$chol <= upper_chol & df$chol >= lower_chol,]
df <- df[df$oldpeak <= upper_oldpeak & df$oldpeak >= lower_oldpeak,]
df <- df[df$caa <= upper_caa & df$caa >= lower_caa,]
```

```{r}
par(mfrow = c(2,2))

boxplot(df$trtbps)
plot(df$trtbps, )

boxplot(df$chol)
plot(df$chol, )

boxplot(df$oldpeak)
plot(df$oldpeak, )

boxplot(df$caa)
plot(df$caa, )
```

******
# Análisis de los datos
******

## Selección de los grupos de datos que se quieren analizar/comparar

En nuestro análisis compararemos los pacientes que sufrieron un ataque cardíaco con los que no. Lo haremos mediante la selección de dos grupos de datos basados en el valor de la variable `output`: pacientes con valor de `output` igual a 1 (pacientes que sufrieron un ataque cardíaco) y pacientes con valor de `output` igual a 0 (pacientes que no sufrieron un ataque cardíaco). Con estos dos grupos seleccionados, podremos aplicar diferentes métodos de análisis estadístico para comparar las diferencias entre los grupos en las diferentes variables del dataset.

- **Correlaciones**: mostraremos las correlaciones y distribuciones de algunas variables para ver como se relacionan entre sí. 
- **Prueba t-Student**: utilizaremos una prueba t de student para comparar el promedio de la edad entre los pacientes que sufrieron un ataque cardíaco y los que no.
- **Regresión logística**: generaremos una regresión logística para analizar la relación entre las variables y el riesgo de sufrir un ataque cardíaco.

Como primer punto  a analizar vamos a verificar si el nivel de colesterol medido en miligramos por decilitro mg/dl tiene una relacion más directa que la edad con la probabilidad de sufrir un ataque cardiaco que la 
Para ellos creamos un nuevo dataset con estas dos variables.


## Comprobación de la normalidad y homogeneidad de la varianza
Como primer punto  a analizar vamos a verificar si existe  una diferencia estadística en el colesterol de hombres y mujeres que tienen una mayor probabilidad de sufrir un ataque al corazón
Para ellos creamos un nuevo dataset con estas dos variables filtrando solo los casos que si tenian posibilidad de un ataque.

```{r}
test <- df[,names(df) %in% c("chol","sex", "output")]
test <- test %>% filter(output == 1)

structure= str(test)
```
Vamos a comprobar la normalidad  la variable chol (colesterol).
```{r}
ks.test(test$chol, pnorm, mean(test$chol), sd(test$chol))
shapiro.test(test$chol)

grid.arrange((ggdensity(test$chol, fill = "lightgreen")),ncol=1)

```
Si nuestra hipótesis nula es que la población está distribuida normalmente, si el p-valor es menor a una significancia de 0.05, entonces rechazamosla hipótesis y 
concluimos  que los datos no cuentan con una distribución normal.
Para nuestras variable colesterol  según Kolmogorov-Smirnov siguen una distribución normal mientras que para que el test de Shapiro-Wilk se rechaza la hipótesis nula y considera que no es así.
Sin embargo por el teorema del limite central, podemos considerar que los datos si siguen una distribución normal.
Podemos apoyar dichos los resultados igualmente a través de un gráfico de densidad podemos ver la distribución de valores para la variable peso y demostrar que toma la forma de campana esperada que indica normalidad.

A continuación para comprobar la homocedasticidad o homogeneidad de la varianza vamos a utilizar el test de Levene y el de Fligner-Killeen que se usa cuando los datos siguen una distribución normal.
Lo haremos comprobando nuestras dos variables colesterol y edad con la siguiente hipotesis.
H0: colesterol = genero
HA: colesterol != genero


```{r}
leveneTest(chol ~ sex, data = test)
fligner.test(chol ~ sex, data = test)
```
Dado que el p-value es mayor que la significancia 0.05, no podemos rechazar la hipótesis nula, H0. La prueba de Levene no es estadísticamente significativa y puede asumir que la varianza de la población de hombres y mujeres es homogénea y puede asumir con seguridad la misma varianza.

## Aplicación de pruebas estadísticas para comparar los grupos de datos.

### Prueba t-Student
Ya que  la normalidad y la homocedasticidad se cumplieron, ahora podemos  aplicar pruebas por contraste de hipótesis de tipo paramétrico como la prueba t Student

```{r}
t.test(chol ~ sex, data = test)

```
El p-valor 0.003927 obtenido de esta prueba t de student es mayor al nivel de significancia, por ende no se observan diferencias estadísticamente significativas entre el sexo del paciente para la variable colesterol.



### Correlaciones

La librería PerformanceAnalytics en R incluye una función llamada chart.Correlation, la cual permite generar un gráfico de relación entre variables con elementos como histogramas, funciones de densidad, líneas de regresión suavizadas y coeficientes de correlación con indicadores de significancia estadística (representado por estrellas, donde ausencia de estrellas indica que la variable no es estadísticamente significativa, mientras que una, dos y tres estrellas indican significancia estadística al nivel del 10%, 5% y 1%, respectivamente).

```{r}
if(!require("PerformanceAnalytics")) install.packages("PerformanceAnalytics"); library(PerformanceAnalytics)

df_num <- df[,sapply(df,is.numeric)]
df_num <- df_num[complete.cases(df_num),]
chart.Correlation(df_num, histogram = TRUE, method = "pearson")

```



### Regresión logística

Queremos analizar la relación entre las variables y el riesgo de sufrir un ataque cardíaco. Para ello utilizaremos una regresión logística, tomando como variable dependiente `output` i un conjunto de variables explicativas de la base de datos, que iremos eliminando o añadiendo según creamos conveniente.
Para poder estimar de forma más objetiva la precisión del modelo, separaremos el conjunto de datos en dos partes: el conjunto de entrenamiento (training) y el conjunto de prueba (testing). Ajustaremos el modelo de regresión logística con el conjunto de entrenamiento, y evaluaremos la precisión con el conjunto de prueba.

#### Generación de los conjuntos de entrenamiento y de test

Generaremos los conjuntos de datos para entrenar el modelo (training) y para testarlo (testing). En este caso, fijaremos el tamaño de la muestra de entrenamiento a un 80% del original.

```{r}
if(!require("caret")) install.packages("caret"); library(caret)

set.seed(888)

# Crear partición de los datos
partition <- createDataPartition(df$output, p = 0.8, list = FALSE)

# Generar conjunto de entrenamiento
training_data <- df[sample(partition), ]

# Generar conjunto de prueba
testing_data <- df[-sample(partition), ]
```

#### Estimación del modelo con el conjunto de entrenamiento e interpretación

Procedemos a seleccionar las variables explicativas y a entrenar el modelo de regresión. Empezamos seleccionando todos los atributos del conjunto de datos.

```{r}
# Seleccionar variables explicativas
explanatory_vars <- names(training_data)[!names(training_data) %in% c("")]

# Estimar modelo de regresión logística
log_model <- glm(output ~ ., data = training_data[, explanatory_vars], family = binomial)

summary(log_model)
```

Los puntos clave de los resultados de una regresión logística son los coeficientes de regresión, los valores de p y las medidas de ajuste del modelo.

1. Los coeficientes de regresión: Estos indican la dirección y magnitud de la relación entre cada variable independiente y la variable dependiente. Un coeficiente positivo indica una relación positiva entre la variable independiente y la variable dependiente, mientras que un coeficiente negativo indica una relación negativa. En este caso, un ejemplo de esto es que `cpNon-anginal pain` tiene un coeficiente positivo de 2.289 y significativamente alto, esto indica que tiene una relación positiva con la probabilidad de sufrir un ataque cardíaco.

2. Los valores de p: Estos indican la probabilidad de que el coeficiente de regresión sea cero (es decir, que no haya relación entre la variable independiente y la variable dependiente). Un valor de p menor que el nivel de significación establecido (generalmente 0.05) indica que es poco probable que el coeficiente sea cero y, por lo tanto, que hay una relación estadísticamente significativa entre la variable independiente y la variable dependiente. En este caso, por ejemplo, el valor de p de 'cpNon-anginal pain' es muy bajo, es decir, 0.000651, lo que indica que es muy probable que exista una relación entre 'cpNon-anginal pain' y la probabilidad de sufrir un ataque cardíaco.

3. Medidas de ajuste del modelo: Estas medidas indican cómo bien el modelo se ajusta a los datos. La desviación residual y el AIC son dos medidas que nos indican qué tan bien se ajusta un modelo a los datos. Una desviación residual y AIC bajos indican que el modelo tiene un mejor ajuste. 

Usaremos el método VIF (Variance Inflation Factor) para estudiar la presencia o no de colinealidad en nuestro modelo de regresión. Un VIF alto indica una alta correlación entre una variable independiente y las demás variables independientes en el modelo. Un valor de VIF superior a 5 suele ser considerado como indicativo de un problema de multicolinealidad. 

```{r}
fiv <- car::vif(log_model)
fiv
```

Los resultados indican que ninguna de las variables independientes del modelo tiene un VIF alto. Esto sugiere que no hay un problema significativo de multicolinealidad entre las variables independientes del modelo. 

Como vimos en el primer modelo, algunas variables no eran estadísticamente significativas (es decir, cuyo valor p sea menor a 0.05), pero como las consideramos importantes en el contexto del análisis, en vez de eliminarlas de dicho modelo entrenaremos uno adicional sin ellas para luego compararlas entre sí.

```{r}
# Seleccionar variables explicativas
explanatory_vars <- names(training_data)[!names(training_data) %in% c("age", "trtbps", "fbs", "restecg", "slp", "chol")]

# Estimar modelo de regresión logística
log_model_2 <- glm(output ~ ., data = training_data[, explanatory_vars], family = binomial)

summary(log_model_2)
```

Como observamos, el primer modelo tiene una desviación residual menor (se ajusta mejor) aunque lo hace a costa de un mayor AIC (utiliza más variables, es menos parsimonioso).

#### Cálculo de las OR (Odds-Ration)

Las odds ratios (OR) son una medida del efecto relativo de una variable explicativa sobre la variable dependiente en un modelo de regresión logística. Una OR mayor que 1 indica que la variable explicativa tiene un efecto positivo en la variable dependiente, mientras que una OR menor que 1 indica un efecto negativo.

```{r}
# Obtener coeficientes del modelo
coefs <- coef(log_model)

# Calcular factores de riesgo o protección
risk_factors <- exp(coefs)

# Imprimir factores de riesgo o protección
risk_factors
```

En el caso observado, se puede ver que la OR para la variable `cpNon-anginal pain` es de 10.94, lo que significa que tener angina atípica tiene 10.94 veces más probabilidades de tener un ataque cardíaco en comparación con tener una angina típica.

#### Matriz de confusión

```{r}
# Obtener predicciones del modelo en el conjunto de prueba
predictions <- predict(log_model, newdata = testing_data, type = "response")

# Crear nueva variable con predicciones del modelo
testing_data$prediction <- ifelse(predictions >= 0.5, 1, 0)

# Comparar predicciones del modelo con variable dependiente del conjunto de prueba
confusion_matrix <- table(testing_data$prediction, testing_data$output)

confusion_matrix

# Calcular tasa de aciertos del modelo
accuracy <- mean(testing_data$prediction == testing_data$output)

# Imprimir tasa de aciertos del modelo
print(accuracy)

sensitivity(confusion_matrix)
specificity(confusion_matrix)
```

Los resultados indican que el modelo de regresión logística tiene una tasa de aciertos del 87%. Esto quiere decir que, de las predicciones del modelo, el 87% son correctas.

Para entender mejor estos resultados, podemos observar la tabla de contingencia que se ha obtenido al comparar la variable prediction con la variable dependiente (output) del conjunto de prueba. Esta tabla nos muestra el número de veces que se ha dado cada combinación de valores en ambas variables.

#### Predicción

```{r}
# Seleccionar variables explicativas
explanatory_vars <- names(df)[!names(df) %in% c("output")]

# Estimar modelo de regresión logística
pacients <- df[c(12, 200), explanatory_vars]

pacients

# Hacer la predicción
prediction_pacients <- predict(log_model, pacients, interval = "confidence", type = "response")

# Mostrar el resultado de la predicción
prediction_pacients
```

En el caso del paciente 12, la probabilidad de tener un ataque cardíaco es de 0.917 o 91.7%. Esto significa que según el modelo, hay un 92.5% de posibilidades de que ese paciente sufra un ataque cardíaco. En el caso del paciente 200, la probabilidad es de 0.011 o 1%. Esto significa que según el modelo, hay un 1% de posibilidades de que ese paciente sufra un ataque cardíaco.

Estas probabilidades no deben interpretarse como una predicción absoluta, sino como una herramienta para identificar pacientes con un mayor riesgo de sufrir un ataque cardíaco y tomar medidas preventivas.

#### Bondad de ajuste

Evaluaremos la bondad del ajuste de nuestro modelo mediante la devianza. Para poder considerar que el modelo es bueno, la devianza residual debe ser menor que la devianza nula.

```{r}
log_model$deviance
log_model$null.deviance

if (log_model$deviance < log_model$null.deviance) {
  print("El modelo log_model tiene un buen ajuste")
} else {
  print("El modelo log_model no tiene un buen ajuste")
}
```

También podemos evaluar la eficacia dl modelo utilizando el test de chi-cuadrado. Si la probabilidad asociada al estadístico del contraste es mayor o igual a 0.05, como en nuestro caso (`prob` = 1), entonces podemos decir que el modelo log_model es eficaz.

```{r}
chisq_observed <- log_model$null.deviance - log_model$deviance
prob <- pchisq(chisq_observed, df = log_model$df.residual, lower.tail = FALSE)

prob
if (prob >= 0.05) {
  print("El modelo log_model es eficaz")
} else {
  print("El modelo log_model no es eficaz")
}
```

#### Curva ROC

```{r}
if(!require("pROC")) install.packages("pROC"); library(pROC)

roc_curve <- roc(testing_data$output, predictions)
plot(roc_curve)

area_under_curve <- auc(roc_curve)
print(paste("Área debajo de la curva:", area_under_curve))

```

El área debajo de la curva (AUC) es una medida de la eficacia del modelo de regresión logística log_model para predecir la variable dependiente `output`. Un AUC de 1 indica un modelo perfecto, mientras que un AUC de 0.5 indica un modelo no mejor que el azar.

En este caso, el área debajo de la curva nos indica que el modelo **log_model** es muy eficaz para predecir la variable dependiente `output`. Este resultado es consistente con los resultados de las medidas de sensibilidad y especifidad que obtuvimos anteriormente.

```{r}
write.csv(df,"Heart_out.csv")
```

# 5. Representación de los resultados a partir de tablas y gráficas. 

Los resultados graficos se representan en los apartados anteriores.

# 6. Resolución del problema. A partir de los resultados obtenidos,


## Conclusiones

En resumen, el análisis realizado en este dataset se enfocó en el diagnóstico de un ataque cardíaco, utilizando una variedad de variables, incluyendo edad, sexo, tipo de dolor en el pecho, presión arterial, colesterol, glicemia, resultados de electrocardiografía, ritmo cardíaco máximo, entre otros.

Para empezar, se realizó un análisis exploratorio de los datos, que permitió entender mejor la distribución de las variables y detectar posibles valores atípicos. Seguidamente, se aplicaron diferentes técnicas estadísticas para analizar los datos, como análisis de correlación, regresión logística, curva ROC y VIF.

En cuanto a los resultados obtenidos, se pudo observar que algunas variables como el sexo, el tipo de dolor en el pecho, la presión arterial, el ritmo cardíaco máximo y el número de vasos principales tienen una relación significativa con el diagnóstico de un ataque cardíaco. También se observó que algunas variables no tienen una relación significativa con el diagnóstico, como la glicemia y los resultados de electrocardiografía.

Analizando a traves de una prueba t student tambien se pudo determinar que no existian diferencias estadísticamente significativas entre el sexo del paciente con la variable colesterol cuando una persona tiene una probabilidad alta de un ataque.

Por otro lado, el modelo de regresión logística proporcionó resultados precisos para la predicción del diagnóstico, como se pudo observar en la curva ROC. Sin embargo, se recomendaría eliminar algunas variables poco significativas antes de aplicar el modelo para mejorar la precisión.

En conclusión, el análisis realizado en este dataset proporciona una comprensión valiosa sobre los factores que contribuyen al diagnóstico de un ataque cardíaco, y puede ser utilizado para desarrollar un sistema de diagnóstico automatizado o mejorar el proceso de diagnóstico actual. Sin embargo, sería recomendable realizar más investigaciones con conjuntos de datos más grandes y variados para validar estos resultados.

# 7 Código.

Ver el codigo en el repositorio: https://github.com/XnetLoL/TD.PRA2

# 8 Video.

Enlace al video en google drive: https://drive.google.com/file/d/1NNcMX8hSdNDo_SEJ0_kgG0_50i5jqRWp/view?usp=sharing


![Contribuciones](firmas.png)

# Bibliografía

- Calvo M, Subirats L, Pérez D (2019). *Introducción a la limpieza y análisis de los datos*. Editorial UOC.
- Julià Minguillón Alfonso, Ramon Caihuelas Quiles. (2021). *Proceso de minería de datos*. Editorial FUOC.
- RASHIK RAHMAN (2021). *Heart Attack Analysis & Prediction Dataset*. https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset?resource=download&select=heart.csv
